{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 2\n",
    "\n",
    "WordNet interface documentation: https://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "import nltk.wsd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.utils.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using loader.py\n",
    "# Apply word tokenization\n",
    "# Apply word lemmatization\n",
    "# Remove stop words\n",
    "\n",
    "# Compare the following two methods for Word Sense Disambiguation:\n",
    "# - The most frequenst sense baseline (First synset)\n",
    "# - NLTK's implementation of Lesk's algorithm (nltk.wsd.lesk)\n",
    "\n",
    "# Develop two additional methods to solve this problem.\n",
    "# - One must use the idea of bootstrapping. This may require you to acquire additional texts in English.\n",
    "#   Since bootstrapping often requires you to specify knowledge about words using heuristics or by\n",
    "#   specifying a seed set, be sure that your method to start the bootstrapping process covers at least\n",
    "#   five different lexical items for which you are performing WSD.\n",
    "# - Any other method of your design. The two methods must be entirely different\n",
    "\n",
    "# Justify decisions about any other parameters to the algorithms, such as what exactly to include\n",
    "#   in the sense and context representations, how to compute overlap, the use of the development set,\n",
    "#   which the started code will load for you.\n",
    "\n",
    "# You may use any heauristic, probabilistic model or any other statistical method that we have discussed\n",
    "#   in class.\n",
    "\n",
    "# Evaluation metric: Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNetError: WSDKey budget%1:21:03:: has no corresponding synset budget.n.03.\n",
      "\tID d012.s022.t005 has no synsets. Dropping.\n",
      "WordNetError: WSDKey budget%1:21:03:: has no corresponding synset budget.n.03.\n",
      "\tID d012.s023.t013 has no synsets. Dropping.\n",
      "Dataset stats:\n",
      "Train instances: 1270/1642 = 77.34%\n",
      "Valid instances: 178/1642 = 10.84%\n",
      "Test instances: 194/1642 = 11.81%\n"
     ]
    }
   ],
   "source": [
    "class WSDInstance:\n",
    "    def __init__(self, my_id, lemma, pos, context, index):\n",
    "        self.id = my_id         # id of the WSD instance\n",
    "        self.lemma = lemma      # lemma of the word whose sense is to be resolved\n",
    "        self.pos = pos          # The position tag\n",
    "        self.context = context  # lemma of all the words in the sentential context\n",
    "        self.index = index      # index of lemma within the context\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        For printing purposes.\n",
    "        '''\n",
    "        return f\"{self.id}\\\\{self.lemma}\\\\{self.pos}\\\\{' '.join(self.context)}\\\\{self.index}\"\n",
    "\n",
    "class WSDKey:\n",
    "    def __init__(self, sense_key):\n",
    "        self.lemma, lex_sense = sense_key.split('%')\n",
    "        self.ss_type, self.lex_filenum, self.lex_id, self.head_word, self.head_id = lex_sense.split(':')\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        For printing purposes.\n",
    "        '''\n",
    "        return f\"{self.lemma}%{self.ss_type}:{self.lex_filenum}:{self.lex_id}:{self.head_word}:{self.head_id}\"\n",
    "\n",
    "def ss_type_to_str(ss_type):\n",
    "    return {\n",
    "        '1': 'n', # Noun\n",
    "        '2': 'v', # Verb\n",
    "        '3': 'a', # Adjective\n",
    "        '4': 's', # Adjective satellite\n",
    "        '5': 'r'  # Adverb\n",
    "    }[ss_type]\n",
    "\n",
    "def to_synset(wsd_key):\n",
    "    ss_type_str = ss_type_to_str(wsd_key.ss_type)\n",
    "    synset_name = f'{wsd_key.lemma}.{ss_type_str}.{wsd_key.lex_id}'\n",
    "    try:\n",
    "        return wn.synset(synset_name)\n",
    "    except WordNetError as e:\n",
    "        print(f'WordNetError: WSDKey {wsd_key} has no corresponding synset {synset_name}.')\n",
    "        return None\n",
    "\n",
    "def load_instances(file_name):\n",
    "    '''\n",
    "    Load two lists of cases to perform WSD on. The structure that is returned is a dict, where\n",
    "    the keys are the ids, and the values are instances of WSDInstance.\n",
    "    '''\n",
    "    tree = ET.parse(file_name)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    train_instances = {}\n",
    "    valid_instances = {}\n",
    "    test_instances = {}\n",
    "    \n",
    "    for text in root:\n",
    "        if text.attrib['id'].startswith('d001'):\n",
    "            instances = test_instances\n",
    "        elif text.attrib['id'].startswith('d007'):\n",
    "            instances = valid_instances\n",
    "        else:\n",
    "            instances = train_instances\n",
    "        for sentence in text:\n",
    "            # construct sentence context\n",
    "            context = [el.attrib['lemma'] for el in sentence]\n",
    "            for i, el in enumerate(sentence):\n",
    "                if el.tag == 'instance':\n",
    "                    my_id = el.attrib['id']\n",
    "                    lemma = el.attrib['lemma']\n",
    "                    pos = el.attrib['pos'][0].lower()\n",
    "                    instances[my_id] = WSDInstance(my_id, lemma, pos, context, i)\n",
    "    return train_instances, valid_instances, test_instances\n",
    "\n",
    "def load_key(file_name):\n",
    "    '''\n",
    "    Load the solutions as dicts.\n",
    "    Key is the id\n",
    "    Value is the list of correct sense keys. \n",
    "    '''\n",
    "    train_key = {}\n",
    "    valid_key = {}\n",
    "    test_key = {}\n",
    "    \n",
    "    for line in open(file_name, encoding=\"utf-8\"):\n",
    "        if len(line) <= 1:\n",
    "            continue\n",
    "        \n",
    "        doc, my_id, sense_keys = line.strip().split(' ', 2)\n",
    "        synsets = [to_synset(WSDKey(sense_key)) for sense_key in sense_keys.split()]\n",
    "        synsets = [synset for synset in synsets if synset is not None]\n",
    "        if len(synsets) is 0:\n",
    "            print(f'\\tID {my_id} has no synsets. Dropping.')\n",
    "            continue\n",
    "        \n",
    "        if doc == 'd001':\n",
    "            test_key[my_id] = synsets\n",
    "        elif doc == 'd007':\n",
    "            valid_key[my_id] = synsets\n",
    "        else:\n",
    "            train_key[my_id] = synsets\n",
    "    return train_key, valid_key, test_key\n",
    "\n",
    "data_folder = 'E:\\\\Repos\\\\comp550\\\\assignment2\\\\data'\n",
    "data_file = f'{data_folder}\\\\multilingual-all-words.en.xml'\n",
    "key_file = f'{data_folder}\\\\wordnet.en.key'\n",
    "\n",
    "train_instances, valid_instances, test_instances = load_instances(data_file)\n",
    "train_key, valid_key, test_key = load_key(key_file)\n",
    "\n",
    "# IMPORTANT: keys contain fewer entries than the instances; need to remove them\n",
    "train_instances = {k:v for (k,v) in train_instances.items() if k in train_key}\n",
    "valid_instances = {k:v for (k,v) in valid_instances.items() if k in valid_key}\n",
    "test_instances = {k:v for (k,v) in test_instances.items() if k in test_key}\n",
    "\n",
    "print('Dataset stats:')\n",
    "total_instances = len(train_instances) + len(valid_instances) + len(test_instances)\n",
    "print(f'Train instances: {len(train_instances)}/{total_instances} = {100*(len(train_instances)/total_instances):.2f}%')\n",
    "print(f'Valid instances: {len(valid_instances)}/{total_instances} = {100*(len(valid_instances)/total_instances):.2f}%')\n",
    "print(f'Test instances: {len(test_instances)}/{total_instances} = {100*(len(test_instances)/total_instances):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file_handler:\n",
    "        return [sentence.rstrip() for sentence in file_handler.readlines()]\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.tag_prefix_dict = {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        return [\n",
    "            self.normalizer.lemmatize(token, pos=self.get_tag_class(tag))\n",
    "            for token, tag in nltk.pos_tag(tokens)\n",
    "        ]\n",
    "    \n",
    "    def get_tag_class(self, tag):\n",
    "        prefix = tag[0].upper()\n",
    "        return self.tag_prefix_dict.get(prefix, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "class Stemmer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        return [\n",
    "            self.normalizer.stem(token)\n",
    "            for token in nltk.word_tokenize(document)\n",
    "        ]\n",
    "\n",
    "def fit_vectorizer(X_data, tokenizer, stop_words, min_df):    \n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop_words,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    vectorizer.fit_transform(X_data)\n",
    "    return vectorizer\n",
    "\n",
    "@sklearn.utils.testing.ignore_warnings(category=sklearn.exceptions.ConvergenceWarning)\n",
    "def random_search(models, search_params, n_datasets=1, n_models=1):\n",
    "    data_sets = []\n",
    "    results = [{} for i in range(n_datasets)]\n",
    "\n",
    "    data_set_variations = [\n",
    "        choose_random_params(search_params['data'])\n",
    "        for i in range(n_datasets)\n",
    "    ]\n",
    "    model_variations = {\n",
    "        model_name: [choose_random_params(search_params['model'][model_name]) for i in range(n_models)]\n",
    "        for model_name in search_params['model'].keys()\n",
    "    }\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        print(f'Data set variation {i+1}/{n_datasets}')\n",
    "\n",
    "        data_params = data_set_variations[i]\n",
    "        data_sets.append(data_params)\n",
    "\n",
    "        print('\\tFitting vectorizer...')\n",
    "        vectorizer = fit_vectorizer(X_train_raw, **data_params)\n",
    "\n",
    "        X_train = vectorizer.transform(X_train_raw)\n",
    "        X_valid = vectorizer.transform(X_valid_raw)\n",
    "        X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "        for model_name, model_class in models.items():\n",
    "            for j in range(n_models):\n",
    "                print(f'\\t{model_name} {j+1}/{n_models}')\n",
    "\n",
    "                model_params = model_variations[model_name][j]\n",
    "\n",
    "                model = model_class(**model_params)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                valid_predictions = model.predict(X_valid)\n",
    "                test_predictions = model.predict(X_test)\n",
    "                \n",
    "                valid_accuracy = sklearn.metrics.accuracy_score(y_valid, valid_predictions)\n",
    "                \n",
    "                # This number is only looked at once at the very end when the best models have been chosen based on validation accuracy\n",
    "                test_accuracy = sklearn.metrics.accuracy_score(y_test, test_predictions)\n",
    "                test_confusion_matrix = sklearn.metrics.confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "                if results[i].get(model_name, None) is None:\n",
    "                    results[i][model_name] = []\n",
    "\n",
    "                results[i][model_name].append({\n",
    "                    'model_params': model_params,\n",
    "                    'valid_accuracy': valid_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'test_confusion_matrix': test_confusion_matrix\n",
    "                })\n",
    "    \n",
    "    return data_sets, results\n",
    "\n",
    "def choose_random_params(parameters):\n",
    "    return {\n",
    "        name: np.random.choice(values)\n",
    "        for name, values in parameters.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostFrequentSense:\n",
    "    def predict(self, instances):\n",
    "        return [wn.synsets(instance.lemma)[0] for key, instance in instances.items()]\n",
    "\n",
    "class NltkLesk:\n",
    "    def predict(self, instances, use_pos):\n",
    "        results = []\n",
    "        for key, instance in instances.items():\n",
    "            pos = instance.pos if use_pos else None\n",
    "            results.append(nltk.wsd.lesk(instance.context, instance.lemma, pos=pos))\n",
    "        return results\n",
    "\n",
    "class YarowskyBootstrapping:\n",
    "    \"\"\"\n",
    "    Yarowsky bootstrapping algorithm implementation\n",
    "    \n",
    "    Paper: https://www.aclweb.org/anthology/P95-1026.pdf\n",
    "\n",
    "    Description of algorithm. For every ambiguous word:\n",
    "        1. Gather all sentences that contain the ambiguous word as an initially untagged training set.\n",
    "        2. Initialise a supervised classifier of n class outputs, where n is the amount of senses the word posesses in WordNet.\n",
    "        3. For each sense of the ambiguous word, tag several corresponding examples using known labels, thus constructing the first seed set. Notes:\n",
    "            * Yarowsky creates his seed set by tagging 2-15% of the ambiguous word examples with their true senses.\n",
    "            * The rest of the untagged examples is called a residual.\n",
    "        4. Train the supervised model on the seed set.\n",
    "        5. Apply the algorithm to all the residual, only keeping high accuracy hits. Add the high accuracy hits to the seed sets.\n",
    "            * The hyper-parameter threshold controls how certain the model needs to be to classify the word\n",
    "        6. Repeat until num_iter is hit or the residual runs out.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, base_model_params, threshold, num_iter):\n",
    "        self.models = {}\n",
    "        self.base_model = base_model\n",
    "        self.base_model_params = base_model_params\n",
    "        self.threshold = threshold\n",
    "        self.num_iter = num_iter\n",
    "    \n",
    "    def _construct_word_instances_dict(instances, synsets_lists):\n",
    "        word_instances_dict = {}\n",
    "        for i in range(len(instances)):\n",
    "            instance = instances[i]\n",
    "            synsets = synsets_lists[i]\n",
    "            if not instance.lemma in word_instances_dict:\n",
    "                word_instances_dict[instance.lemma] = []\n",
    "            word_instances_dict[instance.lemma].append((instance, synsets))\n",
    "        return word_instances_dict\n",
    "    \n",
    "    def train(self, instances, synsets_lists):\n",
    "        pass\n",
    "#         assert type(instances) == list\n",
    "#         assert type(synsets_lists) == list\n",
    "#         assert len(instances) == len(synsets_lists)\n",
    "        \n",
    "#         word_instances_dict = _construct_word_instances_dict(instances, synsets_lists)\n",
    "#         self.models = {lemma: self.base_model(**self.base_model_params) for word_instances_dict.keys()}\n",
    "        \n",
    "#         for lemma, model in self.models.items():\n",
    "#             word_instances_dict[lemma]\n",
    "#             for synset in wn.synsets(lemma):\n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "def calc_accuracy(truth, predictions):\n",
    "    \"\"\"\n",
    "    truth: A list of lists of correct nltk.synsets\n",
    "    predictions: A list of synsets predicted by the model to evaluate\n",
    "    \"\"\"\n",
    "    assert len(truth) == len(predictions)\n",
    "    hits = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] in truth[i]:\n",
    "            hits += 1\n",
    "    return hits/len(truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent sense valid accuracy: 0.33146067415730335\n"
     ]
    }
   ],
   "source": [
    "mfs_model = MostFrequentSense()\n",
    "predictions = mfs_model.predict(valid_instances)\n",
    "accuracy = calc_accuracy([v for k, v in valid_key.items()], predictions)\n",
    "print(f'Most frequent sense valid accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent sense test accuracy: 0.3556701030927835\n"
     ]
    }
   ],
   "source": [
    "predictions = mfs_model.predict(test_instances)\n",
    "accuracy = calc_accuracy([v for k, v in test_key.items()], predictions)\n",
    "print(f'Most frequent sense test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.wsd.lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_stopwords: False, use_pos: False\n",
      "\tAccuracy: 0.39921259842519685\n",
      "remove_stopwords: True, use_pos: False\n",
      "\tAccuracy: 0.4094488188976378\n",
      "remove_stopwords: False, use_pos: True\n",
      "\tAccuracy: 0.4440944881889764\n",
      "remove_stopwords: True, use_pos: True\n",
      "\tAccuracy: 0.494488188976378\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "params = {\n",
    "    'remove_stopwords': [False, True, False, True],\n",
    "    'use_pos' : [False, False, True, True]\n",
    "}\n",
    "\n",
    "nltk_lesk = NltkLesk()\n",
    "for i in range(4):\n",
    "    model_train_instances = copy.deepcopy(train_instances)\n",
    "    \n",
    "    remove_stopwords = params['remove_stopwords'][i]\n",
    "    use_pos = params['use_pos'][i]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        for key, instance in model_train_instances.items():\n",
    "            for word in stopwords_list:\n",
    "                if word in instance.context:\n",
    "                    instance.context.remove(word)\n",
    "    \n",
    "    predictions = nltk_lesk.predict(model_train_instances, use_pos)\n",
    "    \n",
    "    print(f'remove_stopwords: {remove_stopwords}, use_pos: {use_pos}')\n",
    "    print(f'\\tAccuracy: {calc_accuracy([v for k, v in train_key.items()], predictions)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk.wsd.lesk test accuracy:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('nltk.wsd.lesk test accuracy:', end='\\n\\n')\n",
    "remove_stopwords = False\n",
    "use_pos = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yarowsky's algorithm (bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold + num_iter\n",
    "\n",
    "params = {\n",
    "    'remove_stopwords': [False, True, False, True],\n",
    "    'use_pos' : [False, False, True, True]\n",
    "}\n",
    "\n",
    "for i in range(4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The current Numpy installation ('E:\\\\Repos\\\\comp550\\\\venv\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-14cf6e246c40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Repos\\comp550\\venv\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"win32\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0m_win_os_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0m_win_os_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Repos\\comp550\\venv\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m_win_os_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m                    \u001b[1;34m\"See this issue for more information: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                    \"https://tinyurl.com/y3dm3h86\")\n\u001b[1;32m--> 302\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"win32\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The current Numpy installation ('E:\\\\Repos\\\\comp550\\\\venv\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86"
     ]
    }
   ],
   "source": [
    "\n",
    "np.arange(start=0.8, stop=1.01, step=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    'data': {\n",
    "        'tokenizer': [Lemmatizer(), Stemmer()],\n",
    "        'stop_words': ['english', None],\n",
    "        'min_df': [1, 2, 3] # Minimum token frequency\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'eta0': [1e-3, 1e-2, 1e-1], # learning rate\n",
    "            'alpha': [1e-3, 1e-2, 1e-1], # regularization\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'yarowsky_bootstrapping': {\n",
    "            'base_model': sklearn.linear_model.SGDClassifier\n",
    "            'base_model_params': {\n",
    "                'kernel': ['linear'],\n",
    "                'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "                'C': [1e-3, 1e-2, 1e-1], # L2 regularization\n",
    "                'random_state': [random_state]\n",
    "            },\n",
    "            'yarowsky_params': {\n",
    "                'threshold': np.arange(start=0.8, stop=1.01, step=0.05)\n",
    "            }\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(start=10, stop=1000, step=10),\n",
    "            'max_depth': np.append(np.array(None), np.arange(start=1, stop=5, step=2)),\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'yarowsky_bootstrapping': YarowskyBootstrapping,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB\n",
    "}\n",
    "\n",
    "data_sets, results = random_search(models, search_params, n_datasets=3, n_models=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp550 (venv)",
   "language": "python",
   "name": "comp550"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
