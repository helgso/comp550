{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a solution to assignment 1\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import nltk.stem\n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.utils\n",
    "import sklearn.utils.testing\n",
    "import sklearn.exceptions\n",
    "\n",
    "random_state = 1111\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file_handler:\n",
    "        return [sentence.rstrip() for sentence in file_handler.readlines()]\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.tag_prefix_dict = {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        return [\n",
    "            self.normalizer.lemmatize(token, pos=self.get_tag_class(tag))\n",
    "            for token, tag in nltk.pos_tag(tokens)\n",
    "        ]\n",
    "    \n",
    "    def get_tag_class(self, tag):\n",
    "        prefix = tag[0].upper()\n",
    "        return self.tag_prefix_dict.get(prefix, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "class Stemmer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        return [\n",
    "            self.normalizer.stem(token)\n",
    "            for token in nltk.word_tokenize(document)\n",
    "        ]\n",
    "\n",
    "def fit_vectorizer(X_data, tokenizer, stop_words, min_df):    \n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop_words,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    vectorizer.fit_transform(X_data)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'E:/Repos/comp550/assignment1/data'\n",
    "# data_folder = '/home/helgi/repos/comp550/assignment1/data/'\n",
    "positives_file = 'rt-polarity.pos'\n",
    "negatives_file = 'rt-polarity.neg'\n",
    "\n",
    "splits = [0.8, 0.9]\n",
    "\n",
    "negatives_data = [\n",
    "    (document, 'negative')\n",
    "    for document in read_lines(f'{data_folder}/{negatives_file}')\n",
    "]\n",
    "positives_data = [\n",
    "    (document, 'positive')\n",
    "    for document in read_lines(f'{data_folder}/{positives_file}')\n",
    "]\n",
    "all_data = sklearn.utils.shuffle(np.array(negatives_data + positives_data), random_state=random_state)\n",
    "n = all_data.shape[0]\n",
    "\n",
    "X_train_raw, y_train = all_data[:int(splits[0]*n), 0], all_data[:int(splits[0]*n), 1]\n",
    "X_valid_raw, y_valid = all_data[int(splits[0]*n):int(splits[1]*n), 0], all_data[int(splits[0]*n): int(splits[1]*n), 1]\n",
    "X_test_raw, y_test = all_data[int(splits[1]*n):, 0], all_data[int(splits[1]*n):, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set hyper-parameter search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sklearn.utils.testing.ignore_warnings(category=sklearn.exceptions.ConvergenceWarning)\n",
    "def random_search(models, search_params, n_datasets=1, n_models=1):\n",
    "    data_sets = []\n",
    "    results = [{} for i in range(n_datasets)]\n",
    "\n",
    "    data_set_variations = [\n",
    "        choose_random_params(search_params['data'])\n",
    "        for i in range(n_datasets)\n",
    "    ]\n",
    "    model_variations = {\n",
    "        model_name: [choose_random_params(search_params['model'][model_name]) for i in range(n_models)]\n",
    "        for model_name in search_params['model'].keys()\n",
    "    }\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        print(f'Data set variation {i+1}/{n_datasets}')\n",
    "\n",
    "        data_params = data_set_variations[i]\n",
    "        data_sets.append(data_params)\n",
    "\n",
    "        print('\\tFitting vectorizer...')\n",
    "        vectorizer = fit_vectorizer(X_train_raw, **data_params)\n",
    "\n",
    "        X_train = vectorizer.transform(X_train_raw)\n",
    "        X_valid = vectorizer.transform(X_valid_raw)\n",
    "        X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "        for model_name, model_class in models.items():\n",
    "            for j in range(n_models):\n",
    "                print(f'\\t{model_name} {j+1}/{n_models}')\n",
    "\n",
    "                model_params = model_variations[model_name][j]\n",
    "\n",
    "                model = model_class(**model_params)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                valid_predictions = model.predict(X_valid)\n",
    "                test_predictions = model.predict(X_test)\n",
    "                \n",
    "                valid_accuracy = sklearn.metrics.accuracy_score(y_valid, valid_predictions)\n",
    "                \n",
    "                # This number is only looked at once at the very end when the best models have been chosen based on validation accuracy\n",
    "                test_accuracy = sklearn.metrics.accuracy_score(y_test, test_predictions)\n",
    "                test_confusion_matrix = sklearn.metrics.confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "                if results[i].get(model_name, None) is None:\n",
    "                    results[i][model_name] = []\n",
    "\n",
    "                results[i][model_name].append({\n",
    "                    'model_params': model_params,\n",
    "                    'valid_accuracy': valid_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'test_confusion_matrix': test_confusion_matrix\n",
    "                })\n",
    "    \n",
    "    return data_sets, results\n",
    "\n",
    "def choose_random_params(parameters):\n",
    "    return {\n",
    "        name: np.random.choice(values)\n",
    "        for name, values in parameters.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set variation 1/3\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tlinear_support_vector_machine 1/4\n",
      "\tlinear_support_vector_machine 2/4\n",
      "\tlinear_support_vector_machine 3/4\n",
      "\tlinear_support_vector_machine 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n",
      "Data set variation 2/3\n",
      "\tFitting vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tlinear_support_vector_machine 1/4\n",
      "\tlinear_support_vector_machine 2/4\n",
      "\tlinear_support_vector_machine 3/4\n",
      "\tlinear_support_vector_machine 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n",
      "Data set variation 3/3\n",
      "\tFitting vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tlinear_support_vector_machine 1/4\n",
      "\tlinear_support_vector_machine 2/4\n",
      "\tlinear_support_vector_machine 3/4\n",
      "\tlinear_support_vector_machine 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n"
     ]
    }
   ],
   "source": [
    "search_params = {\n",
    "    'data': {\n",
    "        'tokenizer': [Lemmatizer(), Stemmer()],\n",
    "        'stop_words': ['english', None],\n",
    "        'min_df': [1, 2, 3] # Minimum token frequency\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'eta0': [1e-3, 1e-2, 1e-1], # learning rate\n",
    "            'alpha': [1e-3, 1e-2, 1e-1], # regularization\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'linear_support_vector_machine': {\n",
    "            'kernel': ['linear'],\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'C': [1e-3, 1e-2, 1e-1], # L2 regularization\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(start=10, stop=1000, step=10),\n",
    "            'max_depth': np.append(np.array(None), np.arange(start=1, stop=5, step=2)),\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': sklearn.linear_model.SGDClassifier,\n",
    "    'linear_support_vector_machine': sklearn.svm.SVC,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB,\n",
    "    'random_forest': sklearn.ensemble.RandomForestClassifier\n",
    "}\n",
    "\n",
    "data_sets, results = random_search(models, search_params, n_datasets=3, n_models=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset variation 1: {'tokenizer': <__main__.Lemmatizer object at 0x000001D60E7ABDC8>, 'stop_words': None, 'min_df': 2}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.6050656660412758}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.7213883677298312}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.6050656660412758}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.1, 'max_iter': 2, 'random_state': 1111}, 'valid_accuracy': 0.6097560975609756}\n",
      "\tlinear_support_vector_machine:\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 4, 'C': 0.1, 'random_state': 1111}, 'valid_accuracy': 0.48592870544090055}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 1, 'C': 0.1, 'random_state': 1111}, 'valid_accuracy': 0.4821763602251407}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 3, 'C': 0.001, 'random_state': 1111}, 'valid_accuracy': 0.4812382739212008}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 1, 'C': 0.01, 'random_state': 1111}, 'valid_accuracy': 0.47842401500938087}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': 0.797373358348968}\n",
      "\t\t{'model_params': {'alpha': 1.0}, 'valid_accuracy': 0.798311444652908}\n",
      "\t\t{'model_params': {'alpha': 1.0}, 'valid_accuracy': 0.798311444652908}\n",
      "\t\t{'model_params': {'alpha': 0.8}, 'valid_accuracy': 0.798311444652908}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 190, 'max_depth': None, 'random_state': 1111}, 'valid_accuracy': 0.725140712945591}\n",
      "\t\t{'model_params': {'n_estimators': 370, 'max_depth': None, 'random_state': 1111}, 'valid_accuracy': 0.723264540337711}\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 1, 'random_state': 1111}, 'valid_accuracy': 0.5525328330206379}\n",
      "\t\t{'model_params': {'n_estimators': 820, 'max_depth': 1, 'random_state': 1111}, 'valid_accuracy': 0.5478424015009381}\n",
      "\n",
      "Dataset variation 2: {'tokenizer': <__main__.Stemmer object at 0x000001D60E7ABE48>, 'stop_words': 'english', 'min_df': 3}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.5863039399624765}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.7645403377110694}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.5863039399624765}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.1, 'max_iter': 2, 'random_state': 1111}, 'valid_accuracy': 0.575984990619137}\n",
      "\tlinear_support_vector_machine:\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 4, 'C': 0.1, 'random_state': 1111}, 'valid_accuracy': 0.47842401500938087}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 1, 'C': 0.1, 'random_state': 1111}, 'valid_accuracy': 0.48592870544090055}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 3, 'C': 0.001, 'random_state': 1111}, 'valid_accuracy': 0.4812382739212008}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 1, 'C': 0.01, 'random_state': 1111}, 'valid_accuracy': 0.48592870544090055}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': 0.7842401500938087}\n",
      "\t\t{'model_params': {'alpha': 1.0}, 'valid_accuracy': 0.7861163227016885}\n",
      "\t\t{'model_params': {'alpha': 1.0}, 'valid_accuracy': 0.7861163227016885}\n",
      "\t\t{'model_params': {'alpha': 0.8}, 'valid_accuracy': 0.7842401500938087}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 190, 'max_depth': None, 'random_state': 1111}, 'valid_accuracy': 0.7185741088180112}\n",
      "\t\t{'model_params': {'n_estimators': 370, 'max_depth': None, 'random_state': 1111}, 'valid_accuracy': 0.723264540337711}\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 1, 'random_state': 1111}, 'valid_accuracy': 0.5469043151969981}\n",
      "\t\t{'model_params': {'n_estimators': 820, 'max_depth': 1, 'random_state': 1111}, 'valid_accuracy': 0.550656660412758}\n",
      "\n",
      "Dataset variation 3: {'tokenizer': <__main__.Lemmatizer object at 0x000001D60E7ABDC8>, 'stop_words': 'english', 'min_df': 3}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.5703564727954972}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.7589118198874296}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.5703564727954972}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.1, 'max_iter': 2, 'random_state': 1111}, 'valid_accuracy': 0.5637898686679175}\n",
      "\tlinear_support_vector_machine:\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 4, 'C': 0.1, 'random_state': 1111}, 'valid_accuracy': 0.4793621013133208}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 1, 'C': 0.1, 'random_state': 1111}, 'valid_accuracy': 0.48592870544090055}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 3, 'C': 0.001, 'random_state': 1111}, 'valid_accuracy': 0.5140712945590994}\n",
      "\t\t{'model_params': {'kernel': 'linear', 'max_iter': 1, 'C': 0.01, 'random_state': 1111}, 'valid_accuracy': 0.48592870544090055}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': 0.7795497185741088}\n",
      "\t\t{'model_params': {'alpha': 1.0}, 'valid_accuracy': 0.7795497185741088}\n",
      "\t\t{'model_params': {'alpha': 1.0}, 'valid_accuracy': 0.7795497185741088}\n",
      "\t\t{'model_params': {'alpha': 0.8}, 'valid_accuracy': 0.7795497185741088}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 190, 'max_depth': None, 'random_state': 1111}, 'valid_accuracy': 0.7129455909943715}\n",
      "\t\t{'model_params': {'n_estimators': 370, 'max_depth': None, 'random_state': 1111}, 'valid_accuracy': 0.7101313320825516}\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 1, 'random_state': 1111}, 'valid_accuracy': 0.5562851782363978}\n",
      "\t\t{'model_params': {'n_estimators': 820, 'max_depth': 1, 'random_state': 1111}, 'valid_accuracy': 0.5581613508442776}\n"
     ]
    }
   ],
   "source": [
    "for i, params in enumerate(data_sets):\n",
    "    print(f'\\nDataset variation {i+1}: {params}')\n",
    "    model_results = results[i]\n",
    "    for model_name, params in model_results.items():\n",
    "        print(f'\\t{model_name}:')\n",
    "        for j in range(len(params)):\n",
    "            print('\\t\\t{}'.format({\n",
    "                    'model_params': params[j]['model_params'],\n",
    "                    'valid_accuracy': params[j]['valid_accuracy']\n",
    "            }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic_regression test sets accuracies:\n",
      "\t1: 0.7300843486410497\n",
      "\t2: 0.7319587628865979\n",
      "\t3: 0.7544517338331771\n",
      "Best naive_bayes test sets accuracies:\n",
      "\t1: 0.7835051546391752\n",
      "\t2: 0.7497656982193065\n",
      "\t3: 0.7628865979381443\n",
      "Best random_forest test sets accuracies:\n",
      "\t1: 0.6935332708528584\n",
      "\t2: 0.7216494845360825\n",
      "\t3: 0.7244611059044048\n",
      "No best linear_support_vector_machine test sets accuracies\n",
      "Random classifier test set accuracy: 0.5201499531396439\n"
     ]
    }
   ],
   "source": [
    "best_models = {\n",
    "    'logistic_regression': 1,\n",
    "    'naive_bayes': 0,\n",
    "    'random_forest': 0\n",
    "}\n",
    "\n",
    "for model_name, model_index in best_models.items():\n",
    "    print(f'Best {model_name} test sets accuracies:')\n",
    "    for i in range(len(results)):\n",
    "        print(f'\\t{i+1}: {results[i][model_name][model_index][\"test_accuracy\"]}')\n",
    "\n",
    "print(f'No best linear_support_vector_machine test sets accuracies')\n",
    "\n",
    "random_predictions = np.random.choice(['negative', 'positive'], size=y_test.shape[0])\n",
    "random_accuracy = sklearn.metrics.accuracy_score(y_test, random_predictions)\n",
    "print(f'Random classifier test set accuracy: {random_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model (Naive Bayes) confusion matrix:\n",
      "[[433 110]\n",
      " [121 403]]\n"
     ]
    }
   ],
   "source": [
    "print('Best model (Naive Bayes) confusion matrix:')\n",
    "print(results[0]['naive_bayes'][0]['test_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp550 final project (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
