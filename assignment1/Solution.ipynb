{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a solution to assignment 1\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/helgi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/helgi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/helgi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import nltk.stem\n",
    "\n",
    "# Needed for the WordNetLemmatizer\n",
    "import nltk.corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.utils\n",
    "\n",
    "random_state = 111\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file_handler:\n",
    "        return [sentence.rstrip() for sentence in file_handler.readlines()]\n",
    "\n",
    "# data_folder = 'E:/Repos/comp550/assignment1/data'\n",
    "data_folder = '/home/helgi/repos/comp550/assignment1/data/'\n",
    "positives_file = 'rt-polarity.pos'\n",
    "negatives_file = 'rt-polarity.neg'\n",
    "\n",
    "splits = [0.8, 0.9]\n",
    "\n",
    "negatives_data = [\n",
    "    (document, 'negative')\n",
    "    for document in read_lines(f'{data_folder}/{negatives_file}')\n",
    "]\n",
    "positives_data = [\n",
    "    (document, 'positive')\n",
    "    for document in read_lines(f'{data_folder}/{positives_file}')\n",
    "]\n",
    "all_data = sklearn.utils.shuffle(np.array(negatives_data + positives_data), random_state=random_state)\n",
    "n = all_data.shape[0]\n",
    "\n",
    "X_train, y_train = all_data[:int(splits[0]*n), 0], all_data[:int(splits[0]*n), 1]\n",
    "X_valid, y_valid = all_data[int(splits[0]*n):int(splits[1]*n), 0], all_data[int(splits[0]*n): int(splits[1]*n), 1]\n",
    "X_test, y_test = all_data[int(splits[1]*n):, 0], all_data[int(splits[1]*n):, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.tag_prefix_dict = {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        return [\n",
    "            self.normalizer.lemmatize(token, pos=self.get_tag_class(tag))\n",
    "            for token, tag in nltk.pos_tag(tokens)\n",
    "        ]\n",
    "    \n",
    "    def get_tag_class(self, tag):\n",
    "        prefix = tag[0].upper()\n",
    "        return self.tag_prefix_dict.get(prefix, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "class Stemmer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        return [\n",
    "            self.normalizer.stem(token)\n",
    "            for token in nltk.word_tokenize(document)\n",
    "        ]\n",
    "\n",
    "def fit_vectorizer(X_data, tokenizer, stop_words, min_df):    \n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop_words,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    vectorizer.fit_transform(X_data)\n",
    "    return vectorizer\n",
    "\n",
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_params(parameters):\n",
    "    return {\n",
    "        name: np.random.choice(values)\n",
    "        for name, values in parameters.items()\n",
    "    }\n",
    "\n",
    "search_params = {\n",
    "    'data': {\n",
    "        'tokenizer': [Lemmatizer(), Stemmer()],\n",
    "        'stop_words': ['english', None], # Try custom stop words list?\n",
    "        'min_df': [1, 2, 3] # Minimum token frequency\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'learning_rate': ['constant'],\n",
    "            'eta0': [1e-5, 1e-4, 1e-3, 1e-2], # learning rate\n",
    "            'loss': ['log'],\n",
    "            'alpha': [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 2e-1, 4e-1], # regularization\n",
    "            'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'max_iter': np.arange(20), # epochs\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'linear_support_vector_machine': {\n",
    "            'kernel': ['linear'],\n",
    "            'max_iter': np.arange(20), # epochs\n",
    "            'C': [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 2e-1, 4e-1], # L2 regularization\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(1.1, step=0.1),\n",
    "            'fit_prior': [True],\n",
    "            'class_prior': [None]\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(10, 1000, step=10),\n",
    "            'max_depth': np.append(np.array(None), np.arange(16, step=2)),\n",
    "            'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': sklearn.linear_model.SGDClassifier,\n",
    "    'linear_support_vector_machine': sklearn.svm.SVC,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB,\n",
    "    'random_forest': sklearn.ensemble.RandomForestClassifier\n",
    "}\n",
    "\n",
    "data_variations = 1\n",
    "model_variations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variation 1/1\n",
      "\tlogistic_regression 1/1\n",
      "\tlinear_support_vector_machine 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helgi/repos/ift6759-project1/venv/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:573: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/helgi/repos/ift6759-project1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=12).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tnaive_bayes 1/1\n",
      "\trandom_forest 1/1\n"
     ]
    }
   ],
   "source": [
    "data_variation_mapping = {}\n",
    "results = {i: [] for i in range(data_variations)}\n",
    "\n",
    "for i in range(data_variations):\n",
    "    print(f'Data variation {i+1}/{data_variations}')\n",
    "    \n",
    "    data_params = choose_random_params(search_params['data'])\n",
    "    \n",
    "    data_variation_mapping[i] = data_params\n",
    "    \n",
    "    print('Fitting vectorizer...')\n",
    "    vectorizer = fit_vectorizer(X_train, **data_params)\n",
    "    \n",
    "    X_train_model = vectorizer.transform(X_train)\n",
    "    X_valid_model = vectorizer.transform(X_valid)\n",
    "    X_test_model = vectorizer.transform(X_test)\n",
    "\n",
    "    for model_name, model_class in models.items():\n",
    "        for j in range(model_variations):\n",
    "            print(f'\\t{model_name} {j+1}/{model_variations}')\n",
    "            \n",
    "            model_params = choose_random_params(search_params['model'][model_name])\n",
    "            \n",
    "            model = model_class(**model_params)\n",
    "\n",
    "            model.fit(X_train_model, y_train)\n",
    "            predictions = model.predict(X_valid_model)\n",
    "            accuracy = sklearn.metrics.accuracy_score(y_valid, predictions)\n",
    "            \n",
    "            results[i].append({\n",
    "                'model_name': model_name,\n",
    "                'data_params': data_params,\n",
    "                'model_params': model_params,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "\n",
    "# random_predictions = np.random.randint(low=0, high=2, size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_predictions = np.random.randint(low=0, high=2, size=X_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp550 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
