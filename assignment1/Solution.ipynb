{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a solution to assignment 1\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import nltk.stem\n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.utils\n",
    "import sklearn.utils.testing\n",
    "import sklearn.exceptions\n",
    "\n",
    "random_state = 1111\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file_handler:\n",
    "        return [sentence.rstrip() for sentence in file_handler.readlines()]\n",
    "\n",
    "data_folder = 'E:/Repos/comp550/assignment1/data'\n",
    "# data_folder = '/home/helgi/repos/comp550/assignment1/data/'\n",
    "positives_file = 'rt-polarity.pos'\n",
    "negatives_file = 'rt-polarity.neg'\n",
    "\n",
    "splits = [0.8, 0.9]\n",
    "\n",
    "negatives_data = [\n",
    "    (document, 'negative')\n",
    "    for document in read_lines(f'{data_folder}/{negatives_file}')\n",
    "]\n",
    "positives_data = [\n",
    "    (document, 'positive')\n",
    "    for document in read_lines(f'{data_folder}/{positives_file}')\n",
    "]\n",
    "all_data = sklearn.utils.shuffle(np.array(negatives_data + positives_data), random_state=random_state)\n",
    "n = all_data.shape[0]\n",
    "\n",
    "X_train_raw, y_train = all_data[:int(splits[0]*n), 0], all_data[:int(splits[0]*n), 1]\n",
    "X_valid_raw, y_valid = all_data[int(splits[0]*n):int(splits[1]*n), 0], all_data[int(splits[0]*n): int(splits[1]*n), 1]\n",
    "X_test_raw, y_test = all_data[int(splits[1]*n):, 0], all_data[int(splits[1]*n):, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.tag_prefix_dict = {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        return [\n",
    "            self.normalizer.lemmatize(token, pos=self.get_tag_class(tag))\n",
    "            for token, tag in nltk.pos_tag(tokens)\n",
    "        ]\n",
    "    \n",
    "    def get_tag_class(self, tag):\n",
    "        prefix = tag[0].upper()\n",
    "        return self.tag_prefix_dict.get(prefix, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "class Stemmer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        return [\n",
    "            self.normalizer.stem(token)\n",
    "            for token in nltk.word_tokenize(document)\n",
    "        ]\n",
    "\n",
    "def fit_vectorizer(X_data, tokenizer, stop_words, min_df):    \n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop_words,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    vectorizer.fit_transform(X_data)\n",
    "    return vectorizer\n",
    "\n",
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_params(parameters):\n",
    "    return {\n",
    "        name: np.random.choice(values)\n",
    "        for name, values in parameters.items()\n",
    "    }\n",
    "\n",
    "search_params = {\n",
    "    'data': {\n",
    "        'tokenizer': [Lemmatizer(), Stemmer()],\n",
    "        'stop_words': ['english', None],\n",
    "        'min_df': [1, 2, 3] # Minimum token frequency\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'eta0': [1e-3, 1e-2, 1e-1], # learning rate\n",
    "            'alpha': [1e-3, 1e-2, 1e-1], # regularization\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'linear_support_vector_machine': {\n",
    "            'kernel': ['linear'],\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'C': [1e-3, 1e-2, 1e-1], # L2 regularization\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(start=10, stop=1000, step=10),\n",
    "            'max_depth': np.append(np.array(None), np.arange(start=1, stop=5, step=2)),\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': sklearn.linear_model.SGDClassifier,\n",
    "    'linear_support_vector_machine': sklearn.svm.SVC,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB,\n",
    "    'random_forest': sklearn.ensemble.RandomForestClassifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set variation 1/3\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/2\n",
      "\tlogistic_regression 2/2\n",
      "\tlinear_support_vector_machine 1/2\n",
      "\tlinear_support_vector_machine 2/2\n",
      "\tnaive_bayes 1/2\n",
      "\tnaive_bayes 2/2\n",
      "\trandom_forest 1/2\n",
      "\trandom_forest 2/2\n",
      "Data set variation 2/3\n",
      "\tFitting vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlogistic_regression 1/2\n",
      "\tlogistic_regression 2/2\n",
      "\tlinear_support_vector_machine 1/2\n",
      "\tlinear_support_vector_machine 2/2\n",
      "\tnaive_bayes 1/2\n",
      "\tnaive_bayes 2/2\n",
      "\trandom_forest 1/2\n",
      "\trandom_forest 2/2\n",
      "Data set variation 3/3\n",
      "\tFitting vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Repos\\comp550\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlogistic_regression 1/2\n",
      "\tlogistic_regression 2/2\n",
      "\tlinear_support_vector_machine 1/2\n",
      "\tlinear_support_vector_machine 2/2\n",
      "\tnaive_bayes 1/2\n",
      "\tnaive_bayes 2/2\n",
      "\trandom_forest 1/2\n",
      "\trandom_forest 2/2\n"
     ]
    }
   ],
   "source": [
    "@sklearn.utils.testing.ignore_warnings(category=sklearn.exceptions.ConvergenceWarning)\n",
    "def random_search(models, search_params, n_datasets=1, n_models=1):\n",
    "    data_sets = []\n",
    "    results = [{} for i in range(n_datasets)]\n",
    "\n",
    "    data_set_variations = [\n",
    "        choose_random_params(search_params['data'])\n",
    "        for i in range(n_datasets)\n",
    "    ]\n",
    "    model_variations = {\n",
    "        model_name: [choose_random_params(search_params['model'][model_name]) for i in range(n_models)]\n",
    "        for model_name in search_params['model'].keys()\n",
    "    }\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        print(f'Data set variation {i+1}/{n_datasets}')\n",
    "\n",
    "        data_params = data_set_variations[i]\n",
    "        data_sets.append(data_params)\n",
    "\n",
    "        print('\\tFitting vectorizer...')\n",
    "        vectorizer = fit_vectorizer(X_train_raw, **data_params)\n",
    "\n",
    "        X_train = vectorizer.transform(X_train_raw)\n",
    "        X_valid = vectorizer.transform(X_valid_raw)\n",
    "\n",
    "        for model_name, model_class in models.items():\n",
    "            for j in range(n_models):\n",
    "                print(f'\\t{model_name} {j+1}/{n_models}')\n",
    "\n",
    "                model_params = model_variations[model_name][j]\n",
    "\n",
    "                model = model_class(**model_params)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                predictions = model.predict(X_valid)\n",
    "                accuracy = sklearn.metrics.accuracy_score(y_valid, predictions)\n",
    "\n",
    "                if results[i].get(model_name, None) is None:\n",
    "                    results[i][model_name] = []\n",
    "\n",
    "                results[i][model_name].append({\n",
    "                    'model_params': model_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "    \n",
    "    return data_sets, results\n",
    "\n",
    "data_sets, results = random_search(models, search_params, n_datasets=3, n_models=4)\n",
    "# random_predictions = np.random.randint(low=0, high=2, size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenizer': <__main__.Lemmatizer object at 0x000001DD727EE308>, 'stop_words': 'english', 'min_df': 3}\n"
     ]
    }
   ],
   "source": [
    "print(data_sets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_params': {'kernel': 'linear',\n",
       "   'max_iter': 3,\n",
       "   'C': 0.1,\n",
       "   'random_state': 1111},\n",
       "  'accuracy': 0.4812382739212008},\n",
       " {'model_params': {'kernel': 'linear',\n",
       "   'max_iter': 1,\n",
       "   'C': 0.01,\n",
       "   'random_state': 1111},\n",
       "  'accuracy': 0.47842401500938087}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['linear_support_vector_machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_predictions = np.random.randint(low=0, high=2, size=X_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp 550",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
