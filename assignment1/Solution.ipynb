{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a solution to assignment 1\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import nltk.stem\n",
    "\n",
    "# Needed for the WordNetLemmatizer\n",
    "import nltk.corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'E:/Repos/comp550/assignment1/data'\n",
    "positives_file = 'rt-polarity.pos'\n",
    "negatives_file = 'rt-polarity.neg'\n",
    "\n",
    "random_state = 111\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "def read_lines(file_path):\n",
    "    with open(f'{data_folder}/{positives_file}', 'r') as file_handler:\n",
    "        return [sentence.rstrip() for sentence in file_handler.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_data = read_lines(f'{data_folder}/{positives_file}')\n",
    "negatives_data = read_lines(f'{data_folder}/{positives_file}')\n",
    "all_text = \" \".join(positives_data) + \" \".join(negatives_data)\n",
    "all_text_freq_dist = nltk.FreqDist(nltk.word_tokenize(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 13944), (',', 10536), ('the', 10120), ('a', 7690), ('and', 7106), ('of', 6622), ('to', 3940), (\"'s\", 3626), ('is', 3550), ('it', 3348), ('that', 2714), ('in', 2678), ('film', 1798), ('with', 1766), ('as', 1760), ('but', 1568), ('an', 1512), ('its', 1400), ('for', 1348), ('this', 1334)]\n"
     ]
    }
   ],
   "source": [
    "print(all_text_freq_dist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Source: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    Map POS tag to first character lemmatize() accepts\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def remove_rare_words(min_frequency):\n",
    "    return corpus\n",
    "\n",
    "def remove_n_common_stopwords(corpus, n):\n",
    "    return corpus\n",
    "\n",
    "def tokenize(sentence_list):\n",
    "    result = []\n",
    "    for sentence in sentence_list:\n",
    "        result.append(nltk.word_tokenize(sentence))\n",
    "    return result\n",
    "\n",
    "def lemmatize(sentence_tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in sentence_tokens:\n",
    "        words = []\n",
    "        for token in sentence:\n",
    "            words.append(lemmatizer.lemmatize(token, pos=get_wordnet_pos(token)))\n",
    "        sentences.append(words)\n",
    "    return sentences\n",
    "\n",
    "def stem(sentence_tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in sentence_tokens:\n",
    "        words = []\n",
    "        for token in sentence:\n",
    "            words.append(stemmer.stem(token))\n",
    "        sentences.append(words)\n",
    "    return sentences\n",
    "\n",
    "def flatten_2d_list(list_2d):\n",
    "    return [item for sublist in list_2d for item in sublist]\n",
    "\n",
    "def preprocess_dataset(normalization, n_common_stopwords_to_remove, minimum_word_frequency):\n",
    "    assert normalization.lower() in ['lemmatization', 'stemming']\n",
    "    \n",
    "    samples = {\n",
    "        'positive': tokenize(read_lines(f'{data_folder}/{positives_file}')),\n",
    "        'negative': tokenize(read_lines(f'{data_folder}/{positives_file}'))\n",
    "    }\n",
    "    \n",
    "    if normalization == 'lemmatization':\n",
    "        samples['positive'] = lemmatize(samples['positive'])\n",
    "        samples['negative'] = lemmatize(samples['negative'])\n",
    "    else:\n",
    "        samples['positive'] = stem(samples['positive'])\n",
    "        samples['negative'] = stem(samples['negative'])\n",
    "        \n",
    "    freq_dist = nltk.FreqDist(flatten_2d_list(samples['positive']) + flatten_2d_list(samples['negative']))\n",
    "    \n",
    "    # Todo: remove punctuation\n",
    "    # Todo: filter using n_common_stopwords_to_remove\n",
    "    # Todo: filter using minimum_word_frequency\n",
    "    # Todo: return unigram counts -> 0/1 mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_params(parameters):\n",
    "    return {\n",
    "        name: np.random.choice(values)\n",
    "        for name, values in parameters.items()\n",
    "    }\n",
    "\n",
    "search_params = {\n",
    "    'data': {\n",
    "        'normalization': ['lemmatization', 'stemming'],\n",
    "        'n_common_stopwords_to_remove': np.arange(1000, step=10),\n",
    "        'minimum_word_frequency': np.arange(10)\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'learning_rate': ['constant'],\n",
    "            'eta0': [1e-5, 1e-4, 1e-3, 1e-2], # learning rate\n",
    "            'loss': ['log'],\n",
    "            'alpha': [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 2e-1, 4e-1], # regularization\n",
    "            'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'max_iter': np.arange(20), # epochs\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'linear_support_vector_machine': {\n",
    "            'kernel': ['linear'],\n",
    "            'max_iter': np.arange(20), # epochs\n",
    "            'C': [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 2e-1, 4e-1], # L2 regularization\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(1.1, step=0.1),\n",
    "            'fit_prior': [True],\n",
    "            'class_prior': [None]\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(10, 1000, step=10),\n",
    "            'max_depth': np.append(np.array(None), np.arange(16, step=2)),\n",
    "            'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': sklearn.linear_model.SGDClassifier,\n",
    "    'linear_support_vector_machine': sklearn.svm.SVC,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB,\n",
    "    'random_forest': sklearn.ensemble.RandomForestClassifier\n",
    "}\n",
    "\n",
    "data_variations = 1\n",
    "model_variations = 1\n",
    "split = [0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(data_variations):\n",
    "    print(f'Data variation {i+1}/data_variations')\n",
    "    \n",
    "    data_params = choose_random_params(params['data'])\n",
    "    \n",
    "    X, y = preprocess_dataset(**data_params)\n",
    "    X_train, y_train = X[: split[0]], y[: split[0]]\n",
    "    X_valid, y_valid = X[split[0] : split[1]], y[split[0] : split[1]]\n",
    "\n",
    "    for model_name, model_class in models.item():\n",
    "        for j in range(model_variations):\n",
    "            print(f'\\t{model_name} {j+1}/model_variations')\n",
    "            \n",
    "            model_params = choose_random_params(params['model'][model_name])\n",
    "            model = model_class(**model_params)\n",
    "\n",
    "            model.train(X_train, y_train)\n",
    "            predictions = model.predict(X_valid)\n",
    "            accuracy = sklearn.metrics.accuracy_score(y_valid, predictions)\n",
    "            \n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'data_params': data_params,\n",
    "                'model_params': model_params\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "\n",
    "# random_predictions = np.random.randint(low=0, high=2, size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp 550",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
